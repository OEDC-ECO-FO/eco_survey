#  OECD NLP Recommendation Search

Imagine you have **hundreds of policy recommendations** from around the world. They're all written in long paragraphs and talk about very different topics â€” like education, climate, gender, or health.

But thereâ€™s a problem:
> These recommendations donâ€™t come with a label telling us what topic they belong to.

Soâ€¦ how can we **teach a computer** to read these and figure out which topic they match?

This project builds a **semantic search and classification system** for OECD country recommendations using Natural Language Processing (NLP).

We transform raw policy texts into meaningful insights by:
- Understanding the meaning behind recommendations and findings
- Matching each to **predefined policy topics**
- Powering search, dashboards, and analysis â€” without manual labeling

## Project Structure

```
econ_surveys_app/
â”œâ”€â”€ .venv/                  â† Virtual environment
â”œâ”€â”€ app/                    â† Main application folder
â”‚   â”œâ”€â”€ app.py              â† Flask routes (GET/POST)
â”‚   â”œâ”€â”€ wsgi.py             â† WSGI server entry point
â”‚   â”œâ”€â”€ Dockerfile          â† Container config (optional)
â”‚   â”œâ”€â”€ templates/          â† HTML templates
â”‚   â””â”€â”€ utils/              â† (Optional) helper functions
â”œâ”€â”€ data/                   â† Raw and cleaned data
â”‚   â”œâ”€â”€ scraping/           â† Scraping logic
â”‚   â”œâ”€â”€ oecd_findings_and_recommendations.json
â”‚   â””â”€â”€ oecd_recommendations_with_topics.json
â”œâ”€â”€ NLP/                    â† Natural Language Processing logic
â”‚   â”œâ”€â”€ chroma_topic_db/    â† Saved vector embeddings
â”‚   â”œâ”€â”€ models/             â† Local model cache (if needed)
â”‚   â”œâ”€â”€ nlp.ipynb           â† NLP model implementation
â”œâ”€â”€ topic_categories.json   â† Dictionary of topics and definitions
â”œâ”€â”€ requirements.txt        â† Python dependencies
â””â”€â”€ README.md               â† You're here!
```

## ğŸ”§ How to Run This App Locally

### 1. Clone the Repo
```bash
git clone https://github.com/yourname/econ_surveys_app.git
cd econ_surveys_app
```

### 2. Set Up Your Python Environment
```bash
python -m venv .venv
.venv\Scripts\activate  # On Windows
pip install -r requirements.txt
```

### 3. Download the Embedding Model (Optional)
Use this helper to avoid proxy/network issues:
```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")
model.save("models/all-MiniLM-L6-v2")
```

### 4. Run the App
```bash
python app/app.py
```
Or use `wsgi.py` with a server like Gunicorn or waitress.

---

## ğŸ’¡ Notes on Architecture & Flexibility

- The app combines **Elasticsearch** (for metadata filtering) and **ChromaDB** (for semantic ranking).
- Model download and ChromaDB creation are **only run once**.
- Structure supports `app/` folder to encapsulate all web logic (great for modularity).
- You can later split routes into multiple files or add an `api.py` if needed.

---

## Credits
Created by Daniela Ayala (OECD ECO, 2025) with ğŸ’™ for NLP and civic tech.

---

Let us know if you'd like to contribute or expand to other OECD datasets!


